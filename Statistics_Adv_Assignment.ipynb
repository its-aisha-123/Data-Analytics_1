{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4467cd05",
   "metadata": {},
   "source": [
    "1. Explain the properties of the F-distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df93a12",
   "metadata": {},
   "source": [
    "The F-distribution is a continuous probability distribution that arises frequently in the context of statistical hypothesis testing, particularly in the analysis of variance (ANOVA).\n",
    "\n",
    "1. Definition\n",
    "The F-distribution is defined as the ratio of two independent chi-squared (œá¬≤) variables divided by their respective degrees of freedom. If \n",
    "ùëã and ùëå are two independent chi-squared random variables with degrees of freedom ùëë1 and ùëë2, respectively, then the variable ùêπ is defined as:\n",
    "\n",
    "F= (Y/d1)/(X/d2)\n",
    "\n",
    "2. Shape\n",
    "The F-distribution is right-skewed, meaning it has a longer tail on the right side. As the degrees of freedom increase, the distribution becomes less skewed and approaches a normal distribution.\n",
    "3. Degrees of Freedom\n",
    "The F-distribution is characterized by two degrees of freedom: \n",
    "ùëë1 : The degrees of freedom for the numerator (related to the variance of the first sample).\n",
    "ùëë2 : The degrees of freedom for the denominator (related to the variance of the second sample).\n",
    "5. Use in Hypothesis Testing\n",
    "The F-distribution is commonly used to test the equality of variances in different groups (F-test). In ANOVA, it helps determine whether there are any statistically significant differences between the means of three or more independent groups.\n",
    "6. Critical Values\n",
    "Critical values of the F-distribution are used to establish thresholds for making decisions in hypothesis tests. These values depend on the chosen significance level (e.g., Œ± = 0.05) and the corresponding degrees of freedom.\n",
    "7. Non-negative Values\n",
    "The F-distribution only takes non-negative values (i.e., F‚â•0), as it represents a ratio of variances, which cannot be negative.\n",
    "8. Relation to Other Distributions\n",
    "The F-distribution is related to the chi-squared distribution and the normal distribution. Specifically, it can be derived from the ratio of two scaled chi-squared distributions.\n",
    "9. Applications\n",
    "Beyond ANOVA, the F-distribution is used in regression analysis, quality control, and other statistical modeling techniques where variance comparison is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345eeb2",
   "metadata": {},
   "source": [
    "2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d676f937",
   "metadata": {},
   "source": [
    "The F-distribution is primarily used in various statistical tests that involve the comparison of variances or the assessment of multiple group means. Here are the main types of statistical tests where the F-distribution is employed, along with reasons for its appropriateness:\n",
    "\n",
    "1. Analysis of Variance (ANOVA)\n",
    "Use: ANOVA tests whether there are statistically significant differences between the means of three or more groups.\n",
    "Appropriateness: ANOVA compares the variance between the groups to the variance within the groups. The F-statistic, which follows an F-distribution, is calculated as the ratio of these variances. This test assesses whether the group means are equal or if at least one group mean differs significantly.\n",
    "2. F-test for Equality of Variances\n",
    "Use: This test compares the variances of two populations to determine if they are significantly different.\n",
    "Appropriateness: The F-test uses the ratio of two sample variances (which are chi-squared distributed) to create an F-statistic. If the null hypothesis (that the two population variances are equal) is true, the F-statistic will follow an F-distribution.\n",
    "3. Regression Analysis\n",
    "Use: In multiple regression, the F-test is used to determine if the model as a whole is statistically significant.\n",
    "Appropriateness: The F-statistic assesses the ratio of the variance explained by the regression model to the variance not explained (residual variance). If the null hypothesis (that all regression coefficients are zero) is true, this statistic will follow an F-distribution.\n",
    "4. General Linear Models\n",
    "Use: In the context of general linear models, the F-test can be used to assess the significance of predictors.\n",
    "Appropriateness: Similar to regression analysis, the F-test evaluates the variance explained by the model relative to the unexplained variance. The distribution of the test statistic under the null hypothesis is F-distributed.\n",
    "5. Analysis of Covariance (ANCOVA)\n",
    "Use: ANCOVA adjusts the means of the dependent variable for one or more covariates before comparing group means.\n",
    "Appropriateness: The F-statistic is used to compare the adjusted means across groups, and it follows an F-distribution when the assumptions are met.\n",
    "6. Multivariate Analysis of Variance (MANOVA)\n",
    "Use: MANOVA extends ANOVA to multiple dependent variables.\n",
    "Appropriateness: The F-test is used to evaluate the hypothesis that the mean vectors of different groups are equal, relying on the properties of the F-distribution.\n",
    "\n",
    "\n",
    "Reasons for Appropriateness of the F-distribution::\n",
    "Ratio of Variances: Many tests using the F-distribution involve ratios of variances. The distribution models the variability expected under the null hypothesis of no effect or no difference.\n",
    "\n",
    "Independence: The F-distribution assumes that the two samples being compared are independent, which is a key condition in many statistical tests.\n",
    "\n",
    "Non-negative Values: Since variances cannot be negative, the non-negative nature of the F-distribution makes it suitable for these applications.\n",
    "\n",
    "Distributional Properties: The F-distribution's shape (right-skewed) is well-suited for hypothesis testing, especially when sample sizes are sufficiently large, leading to an approximate normality in the sampling distribution of the test statistic.\n",
    "\n",
    "Degrees of Freedom: The dependence of the F-distribution on two sets of degrees of freedom allows it to adjust for different sample sizes and the complexity of the models being tested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f487856f",
   "metadata": {},
   "source": [
    "3. What are the key assumptions required for conducting an F-test to compare the variances of two \n",
    "   populations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9af1fe",
   "metadata": {},
   "source": [
    "When conducting an F-test to compare the variances of two populations, several key assumptions must be met to ensure the validity of the test results. These assumptions include:\n",
    "\n",
    "1. Independence of Samples\n",
    "The two samples being compared must be independent of each other. This means that the selection or measurement of one sample does not influence the selection or measurement of the other sample.\n",
    "2. Normality\n",
    "The data in each group should be approximately normally distributed. While the F-test is somewhat robust to deviations from normality, especially with larger sample sizes, severe departures from normality can affect the accuracy of the test results.\n",
    "3. Continuous Data\n",
    "The data being analyzed should be continuous. The F-test is not appropriate for categorical or ordinal data.\n",
    "4. Homogeneity of Variances\n",
    "Although the F-test is designed to compare variances, it assumes that the populations being compared have variances that are equal under the null hypothesis. This is known as the homogeneity of variances assumption. If this assumption is violated, the F-test results may be misleading.\n",
    "5. Random Sampling\n",
    "The samples should be drawn randomly from their respective populations. This ensures that the samples are representative and that the test results can be generalized to the larger population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e330e6cf",
   "metadata": {},
   "source": [
    "4. What is the purpose of ANOVA, and how does it differ from a t-test? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f31d09",
   "metadata": {},
   "source": [
    "Purpose of ANOVA::\n",
    "Comparing Multiple Group Means:\n",
    "\n",
    "ANOVA is primarily used to determine whether there are statistically significant differences among the means of three or more groups. It helps assess whether at least one group mean is different from the others.\n",
    "Variance Partitioning:\n",
    "\n",
    "ANOVA analyzes variance in the data by partitioning it into components: variance between groups and variance within groups. This helps to understand the sources of variability in the data.\n",
    "Testing Overall Effects:\n",
    "\n",
    "ANOVA is useful for testing the effect of one or more independent variables on a dependent variable when there are multiple levels of the independent variable(s).\n",
    "\n",
    "\n",
    "Differences from a t-test::\n",
    "\n",
    "Number of Groups:\n",
    "ANOVA is used to compare the means of three or more groups.\n",
    "A t-test is used to compare the means of two groups only.\n",
    "\n",
    "Hypothesis Testing:\n",
    "ANOVA tests whether there is at least one significant difference among group means.\n",
    "A t-test assesses whether the means of two groups are equal.\n",
    "\n",
    "Statistical Model:\n",
    "ANOVA utilizes an F-statistic, which is based on the ratio of variances (between-group variance to within-group variance).\n",
    "A t-test uses a t-statistic, which is based on the difference between group means relative to the variability of the groups.\n",
    "\n",
    "Complexity:\n",
    "ANOVA can handle more complex experimental designs, such as factorial designs, which involve multiple independent variables.\n",
    "A t-test is more straightforward and is typically used in simpler experiments with only one independent variable.\n",
    "\n",
    "Post-hoc Analysis:\n",
    "If ANOVA indicates significant differences, post-hoc tests (e.g., Tukey‚Äôs HSD) are required to determine which specific group means differ.\n",
    "A t-test does not require post-hoc analysis since it compares only two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba6b09",
   "metadata": {},
   "source": [
    "5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more \n",
    "   than two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07191da",
   "metadata": {},
   "source": [
    "Using a one-way ANOVA instead of multiple t-tests when comparing more than two groups is recommended for several key reasons:\n",
    "\n",
    "When to Use One-Way ANOVA\n",
    "Comparing Three or More Groups:\n",
    "\n",
    "You should use a one-way ANOVA when you have three or more independent groups that you want to compare on a single dependent variable.\n",
    "Experimental Design:\n",
    "\n",
    "It is appropriate when the research design involves one independent variable with multiple levels (e.g., different treatments or conditions) and a continuous dependent variable.\n",
    "Why Use One-Way ANOVA\n",
    "Control Type I Error Rate:\n",
    "\n",
    "Conducting multiple t-tests increases the likelihood of Type I errors (incorrectly rejecting the null hypothesis). Each t-test has its own significance level (e.g., Œ± = 0.05), so performing multiple tests inflates the overall error rate. One-way ANOVA maintains a single significance level across all comparisons, reducing the risk of false positives.\n",
    "Efficiency:\n",
    "\n",
    "One-way ANOVA is more efficient than performing multiple t-tests, as it consolidates the comparisons into one analysis. This not only saves time but also simplifies interpretation.\n",
    "Comprehensive Analysis:\n",
    "\n",
    "ANOVA assesses overall differences among group means, allowing researchers to determine if there are significant differences without needing to conduct separate tests for each pair of groups.\n",
    "Variance Analysis:\n",
    "\n",
    "One-way ANOVA evaluates the variance between groups relative to the variance within groups. This helps identify whether the observed differences are likely due to the treatment or random variation.\n",
    "Post-hoc Testing:\n",
    "\n",
    "If one-way ANOVA indicates significant differences among group means, post-hoc tests (e.g., Tukey‚Äôs HSD) can be used to determine specifically which groups differ. This is more systematic than conducting multiple pairwise t-tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f7957",
   "metadata": {},
   "source": [
    "6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. \n",
    "   How does this partitioning contribute to the calculation of the F-statistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99edffdf",
   "metadata": {},
   "source": [
    "n ANOVA (Analysis of Variance), the total variance in the data is partitioned into two components: between-group variance and within-group variance. This partitioning is essential for understanding the sources of variability in the data and contributes to the calculation of the F-statistic. Here‚Äôs how this process works:\n",
    "\n",
    "1. Total Variance\n",
    "The total variance in a dataset can be defined as the variability of all data points around the overall mean. \n",
    "\n",
    "2. Partitioning Variance\n",
    "The total variance is partitioned into two components:\n",
    "\n",
    "a. Between-Group Variance (Variation due to Treatment)\n",
    "This component measures how much the group means differ from the overall mean. It reflects the variability attributed to the different treatments or groups.\n",
    "b. Within-Group Variance (Error Variation)\n",
    "This component measures the variability of observations within each group around their respective group means. It reflects the natural variability among the individual observations within the groups.\n",
    "\n",
    "3. Calculation of the F-statistic\n",
    "The F-statistic in ANOVA is calculated as the ratio of the mean square between groups to the mean square within groups. This is expressed as:\n",
    "\n",
    "F= MS between / MS within\n",
    " \n",
    "where:\n",
    "ùëÄùëÜ between: (mean square for between-group variance),\n",
    "ùëÄS within : (mean square for within-group variance).\n",
    "\n",
    "\n",
    "Contribution of Variance Partitioning to the F-statistic::\n",
    "Interpretation:\n",
    "\n",
    "The F-statistic compares the amount of variation explained by the group differences (between-group variance) to the amount of variation within the groups (within-group variance). A higher F-statistic indicates that the between-group variance is greater than the within-group variance, suggesting that the group means are likely different.\n",
    "Hypothesis Testing:\n",
    "\n",
    "The F-statistic is used to test the null hypothesis that all group means are equal. If the F-statistic is significantly large (based on a critical value from the F-distribution), we reject the null hypothesis and conclude that at least one group mean is different.\n",
    "Robustness:\n",
    "\n",
    "By partitioning variance, ANOVA provides a systematic way to evaluate the effect of the independent variable(s) on the dependent variable while controlling for variability within groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eef6f4",
   "metadata": {},
   "source": [
    "7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key \n",
    "   differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e9c55",
   "metadata": {},
   "source": [
    "The classical (frequentist) approach to ANOVA and the Bayesian approach differ significantly in their philosophies and methodologies. Here are the key differences in terms of handling uncertainty, parameter estimation, and hypothesis testing:\n",
    "\n",
    "1. Handling Uncertainty\n",
    "Frequentist Approach:\n",
    "\n",
    "Uncertainty is expressed through concepts like confidence intervals and p-values. In this framework, probability is defined as the long-run frequency of events. For instance, a 95% confidence interval means that if the same experiment were repeated many times, approximately 95% of the calculated intervals would contain the true parameter.\n",
    "In ANOVA, the focus is on assessing whether there is a significant effect by comparing variances between and within groups.\n",
    "Bayesian Approach:\n",
    "\n",
    "Uncertainty is quantified using probability distributions, which provide a direct measure of uncertainty about parameters. Bayesian methods treat parameters as random variables with their own probability distributions.\n",
    "In Bayesian ANOVA, prior distributions are assigned to parameters based on previous knowledge or beliefs, and posterior distributions are obtained after observing the data.\n",
    "2. Parameter Estimation\n",
    "Frequentist Approach:\n",
    "\n",
    "Parameters are estimated using point estimates (e.g., sample means) and confidence intervals. The estimates are considered fixed but unknown quantities.\n",
    "Frequentist methods often focus on maximum likelihood estimation (MLE) to estimate parameters, providing estimates that maximize the likelihood of observing the data given the parameters.\n",
    "Bayesian Approach:\n",
    "\n",
    "Parameters are treated as random variables, and estimation is done through posterior distributions, which combine prior information with observed data.\n",
    "Bayesian methods provide not just point estimates but also credible intervals, which give a range of values that can contain the true parameter with a certain probability (e.g., a 95% credible interval means there's a 95% probability that the parameter lies within that interval given the data and prior).\n",
    "3. Hypothesis Testing\n",
    "Frequentist Approach:\n",
    "\n",
    "Hypothesis testing is conducted using p-values. A null hypothesis (e.g., all group means are equal) is tested against an alternative hypothesis (e.g., at least one group mean differs). A small p-value (typically less than 0.05) leads to rejection of the null hypothesis.\n",
    "The approach does not provide probabilities for hypotheses; instead, it focuses on whether the data are consistent with the null hypothesis.\n",
    "Bayesian Approach:\n",
    "\n",
    "Bayesian hypothesis testing involves comparing the posterior probabilities of different hypotheses (e.g., the null versus the alternative). It provides a probability statement about the hypotheses based on the observed data and prior beliefs.\n",
    "The Bayesian framework allows for more flexible modeling of hypotheses, including the possibility of incorporating prior information and directly assessing the probability of the null hypothesis.\n",
    "4. Interpretation of Results\n",
    "Frequentist Approach:\n",
    "\n",
    "Results are interpreted in terms of long-term frequencies and confidence intervals, often leading to a binary decision (reject or fail to reject the null hypothesis).\n",
    "The interpretation of p-values can be misused or misunderstood, leading to debates about the validity of significance thresholds.\n",
    "Bayesian Approach:\n",
    "\n",
    "Results are interpreted in terms of probabilities, which can be more intuitive. For instance, one might say there is a 70% probability that a particular group mean is greater than another, based on the posterior distribution.\n",
    "Bayesian methods can provide richer insights by allowing researchers to incorporate prior knowledge and update beliefs as new data are collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b99f0",
   "metadata": {},
   "source": [
    "8. Question: You have two sets of data representing the incomes of two different professions1\n",
    "V Profession A: [48, 52, 55, 60, 62'\n",
    "V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions' \n",
    "incomes are equal. What are your conclusions based on the F-test?\n",
    "\n",
    "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
    "\n",
    "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5552648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc056a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the two professions\n",
    "profession_a = np.array([48, 52, 55, 60, 62])\n",
    "profession_b = np.array([45, 50, 55, 52, 47])\n",
    "\n",
    "# Calculate the sample variances\n",
    "var_a = np.var(profession_a, ddof=1)  # Sample variance for Profession A\n",
    "var_b = np.var(profession_b, ddof=1)  # Sample variance for Profession B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9540cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the F-statistic\n",
    "f_statistic = var_a / var_b\n",
    "\n",
    "# Degrees of freedom\n",
    "df_a = len(profession_a) - 1  # degrees of freedom for Profession A\n",
    "df_b = len(profession_b) - 1  # degrees of freedom for Profession B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "619d482f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 2.09\n",
      "p-value: 0.247\n",
      "Fail to reject the null hypothesis: the variances are equal.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the p-value\n",
    "p_value = 1 - stats.f.cdf(f_statistic, df_a, df_b)\n",
    "\n",
    "# Print the results\n",
    "print(f\"F-statistic: {f_statistic:.2f}\")\n",
    "print(f\"p-value: {p_value:.3f}\")\n",
    "\n",
    "# Conclusion based on the p-value\n",
    "alpha = 0.05  # significance level\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: the variances are significantly different.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: the variances are equal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512cdcdf",
   "metadata": {},
   "source": [
    "9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in \n",
    "average heights between three different regions with the following data1\n",
    "V Region A: [160, 162, 165, 158, 164'\n",
    "V Region B: [172, 175, 170, 168, 174'\n",
    "V Region C: [180, 182, 179, 185, 183'\n",
    "V Task: Write Python code to perform the one-way ANOVA and interpret the results\f",
    "\n",
    "V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5be456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 67.87\n",
      "p-value: 2.87066e-07\n",
      "Reject the null hypothesis: there are statistically significant differences in average heights among the regions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Data for the three regions\n",
    "region_a = np.array([160, 162, 165, 158, 164])\n",
    "region_b = np.array([172, 175, 170, 168, 174])\n",
    "region_c = np.array([180, 182, 179, 185, 183])\n",
    "\n",
    "# Combine the data into a list for the ANOVA function\n",
    "data = [region_a, region_b, region_c]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(*data)\n",
    "\n",
    "# Print the results\n",
    "print(f\"F-statistic: {f_statistic:.2f}\")\n",
    "print(f\"p-value: {p_value:.5e}\")\n",
    "\n",
    "# Conclusion based on the p-value\n",
    "alpha = 0.05  # significance level\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: there are statistically significant differences in average heights among the regions.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: there are no statistically significant differences in average heights among the regions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95786986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
